{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5884a7f-3a86-48f1-b4ed-ef3571dd46b2",
   "metadata": {},
   "source": [
    "## Distant reading study week 3 (VT-23)\n",
    "\n",
    "### Learning material 3a: Processing several files (txt)\n",
    "\n",
    "Matti La Mela\n",
    "\n",
    "This Notebook introduces us to reading several files with glob (global) and processing them in Python, which is a way to scale up your research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79ce28-cab9-4ff9-b42d-5345ca2caaf4",
   "metadata": {},
   "source": [
    "### 1. Reading several text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e24a2058-96d3-41b3-be30-e2588aa169fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../text_week3/dhq_corpus_complete_2007_2020\\dhq-2007-000001-Drucker-Philosophy.txt\n",
      "../text_week3/dhq_corpus_complete_2007_2020\\dhq-2007-000002-Howard-Interpretative.txt\n",
      "../text_week3/dhq_corpus_complete_2007_2020\\dhq-2007-000003-VandeCreek-Webs.txt\n",
      "../text_week3/dhq_corpus_complete_2007_2020\\dhq-2007-000004-Patrik-Encoding.txt\n",
      "../text_week3/dhq_corpus_complete_2007_2020\\dhq-2007-000005-Wolff-Reading.txt\n"
     ]
    }
   ],
   "source": [
    "# glob allows us to search for filenames in a path. We can use the result for reading the files name by name.\n",
    "\n",
    "import glob\n",
    "\n",
    "# Let us see what text files we have in our dhq_corpus_complete_2007_2020 path. If we use the wild card *.txt, we get all the files that are of txt file\n",
    "# extension. Glob creates a list of the file names:\n",
    "\n",
    "list_files = glob.glob(\"../text_week3/dhq_corpus_complete_2007_2020/*\")\n",
    "\n",
    "# Let's print the first five filenames:\n",
    "\n",
    "for filename in list_files[0:5]:\n",
    "    print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14bd9a15-dc48-4ebd-839e-6fedeb783753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We open every file in our list_files in the for-loop and do other relevant operations with them (e.g. data cleaning could be done here).\n",
    "\n",
    "# In this example we save our files as strings to a list (\"texts\"), where we could then continue working with them.\n",
    "\n",
    "texts = []\n",
    "\n",
    "for filename in list_files:\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        article = file.read()\n",
    "        texts.append(article)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2faf9fdb-bfda-45e1-86f8-6b892023c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philosophy and Digital Humanities: A review of Willard McCarty, Humanities\n",
      "      Computing (London and NY: Palgrave, 2005)\n",
      "Johanna Drucker\n",
      "3 April 2007\n",
      "\n",
      "Humanities computing is still a fledgling disci\n"
     ]
    }
   ],
   "source": [
    "# We have now the whole dhq corpus stored to a list. \n",
    "\n",
    "# Let's print the first 200 characters of the first article in our list: first index is for the list element [0], and the second for the range in the string [0:200]\n",
    "\n",
    "print(texts[0][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce48b19-7bf4-4ec3-ab2b-118d78d1ad7a",
   "metadata": {},
   "source": [
    "### 2. Reading files and processing them in SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c758200-d7f1-4c2c-adf1-d8c30e4040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we open multiple files and process them in SpaCy. If we have lots of files it might not possible to save them into variables\n",
    "# due to memory restrictions. We process the files hen we open them, save the information we need, and then continue with the next file.\n",
    "\n",
    "# Let's import spacy and load the language model.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc2b46ff-94f7-47f5-8ea1-e975a163823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We produce a list of the filenames with glob for our dhq corpus, thus, all the .txt files:\n",
    "\n",
    "import glob\n",
    "\n",
    "list_files = glob.glob(\"./dhq_corpus_complete_2007_2020/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c977787-01b9-4ff0-bbda-2aa03acaf04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000001-Drucker-Philosophy.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 001-Drucker-Philosophy.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000002-Howard-Interpretative.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 002-Howard-Interpretative.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000003-VandeCreek-Webs.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 003-VandeCreek-Webs.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000004-Patrik-Encoding.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 004-Patrik-Encoding.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000005-Wolff-Reading.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 005-Wolff-Reading.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000006-Raben-Tenure.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 006-Raben-Tenure.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000007-Flanders-Welcome.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 007-Flanders-Welcome.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000008-Raben-Introducing.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 008-Raben-Introducing.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000009-Jerz-Somewhere.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 009-Jerz-Somewhere.txt\n",
      "\n",
      "Processing file ./dhq_corpus_complete_2007_2020\\dhq-2007-000010-Eve-All.txt through spacy nlp pipeline\n",
      "Done! Storing information for : 010-Eve-All.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, we want store three kinds of information to lists and save them for later use:\n",
    "\n",
    "# 1. the filenames of our files we open: \"texts_filenames\"\n",
    "# 2. the lengths of our texts: \"texts_length\"\n",
    "# 3. all the noun lemmas: \"texts_noun_lemmas\"\n",
    "\n",
    "texts_filenames = []\n",
    "texts_length = []\n",
    "texts_noun_lemmas = []\n",
    "\n",
    "# We will process only ten files from the dhq corpus. We can do this by limiting our for loop with the index range [0:10]\n",
    "# This is a good way to test that everything works, and when everything works, we could process all the files (by removing the index range).\n",
    "\n",
    "# In the for-loop, we process the files one by one, when we open them:\n",
    "\n",
    "for filename in list_files[0:10]:\n",
    "    \n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        \n",
    "        text = file.read() # We read the file as a string to the variable \"text\"\n",
    "        \n",
    "        text_doc = nlp(text, disable=[\"parser\", \"ner\"]) # We process \"text\" with SpaCy. We disable the parser and NER processes, as we want to save some time\n",
    "        \n",
    "        print(\"Processing file \" + filename + \" through spacy nlp pipeline\") # Let's print some info to the user so we know where we are\n",
    "        \n",
    "        # we want to store all the lemmas of nouns found in the article. We do this with another for-loop, like in the week 2 exercises.\n",
    "        \n",
    "        lemmas = []\n",
    "        \n",
    "        for token in text_doc:\n",
    "            if token.is_alpha and token.is_stop == False:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    lemmas.append(token.lemma_)\n",
    "\n",
    "        # We are also interested in the length of the original text, which we save in the list \"texts_length\".\n",
    "        # Finally, we store the filename in the list \"filenames\". We store the information to all three lists at the same time,\n",
    "        # so we could iterate lists simultaneously and always get the corresponding values (e.g. filename - length - lemmas)\n",
    "\n",
    "        # The filename with the path is pretty long, so let's take only the name of the file. We could use regular expressions, but we know that\n",
    "        # the beginning of the path name is always the same. We can simply use the index from character 45, thus index 44 (calculated manually!)\n",
    "        \n",
    "        print(\"Done! Storing information for : \" + filename[44:] + \"\\n\")\n",
    "        \n",
    "        texts_noun_lemmas.append(lemmas)\n",
    "        texts_filenames.append(filename[44:])\n",
    "        texts_length.append(len(text))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26fe517f-1038-46a2-b2c1-259579dcf984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article filename is : 001-Drucker-Philosophy.txt\n",
      "The first ten noun lemmas are : ['philosophy', 'review', 'computing', 'fledgling', 'discipline', 'spite', 'claim', 'lineage', 'decade', 'labor']\n",
      "The length of the first article is : 15012\n"
     ]
    }
   ],
   "source": [
    "# When we are ready, we should have the lemmas of nouns from the articles in a list, the filenames in another, and the lengths in the third.\n",
    "\n",
    "# let's print the filename of the first article, the first ten noun lemmas from it, and also its length.\n",
    "\n",
    "print(\"The article filename is : \" + texts_filenames[0])\n",
    "print(\"The first ten noun lemmas are : \" + str(texts_noun_lemmas[0][0:10]))\n",
    "print(\"The length of the first article is : \" + str(texts_length[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "199475aa-eda6-4d5d-8c65-76d39a43b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 lemmas in the first article: [('humanity', 17), ('knowledge', 14), ('way', 11), ('study', 10), ('field', 9)]\n"
     ]
    }
   ],
   "source": [
    "# We could take our list to pandas for counting, but we use here another way for simple counting: the Counter that we get from\n",
    "# the Python module \"collections\" is handy for this (though further operations might be easier to do in pandas)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "example_article = texts_noun_lemmas[0] # We assign the lemmas of the first article to our variable \"example_article\"\n",
    "\n",
    "print (\"Top 5 lemmas in the first article: \" + str(Counter(example_article).most_common(5))) # here we print the 5 most common lemmas in our string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d37073-14eb-490a-ba3e-d33fcab8081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These steps offer you possibilities for processing several files. Another option would be to simply store all the information to one list\n",
    "# and then work with that.\n",
    "\n",
    "# NB! spacy tokens take a lot of memory as they contain much information. It is thus good to process only one text (or a part of a text\n",
    "# at time), so we don't get memory errors. The default maximum text size that SpaCy can process is 1M characters. We have an example of processing too big\n",
    "# files in Material_3b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e9932a7-7f14-4df9-a259-60da07021db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's write our lists to a csv, so we can continue our work in Excel for example.\n",
    "\n",
    "# We use the module called csv:\n",
    "\n",
    "import csv\n",
    "\n",
    "i = 0 # We use i as our index when we iterate the texts we have processed\n",
    "\n",
    "with open(\"./lemmas_output.csv\", \"w\", encoding=\"utf-8\") as file: # We save our output to the file lemmas_output.csv, we write the file \"w\"\n",
    "    \n",
    "    # We create a writer object \"write\" that is able to write things to the \"file\" that we have created above.\n",
    "    # The default delimiter (or separator) for writing csv output is \",\" but as we have commas in the elements\n",
    "    # that we want to write, we use another delimiter \";\". For tab the delimiter is \"\\t\"\n",
    "\n",
    "    write = csv.writer(file, delimiter = \";\")\n",
    "    \n",
    "    write.writerow([\"Filename\", \"Article length\", \"Top five lemmas\", \"All lemmas of nouns\"])      # Let's write first a row with the column names to the csv.\n",
    "    \n",
    "    for files in texts_filenames: # In this for-loop, We iterate all the files we have processed, and write them to the csv\n",
    "\n",
    "        lemmas_string = \" \".join(texts_noun_lemmas[i]) # We join our list of lemmas into a string variable \"lemmas_string\"\n",
    "        count = Counter(texts_noun_lemmas[i]).most_common(5) # We store the top five lemmas to variable \"count\"\n",
    "\n",
    "        # we write four strings: filename, length, \"count\" which stores top 5 lemmas with Counter, and then the lemmas:\n",
    "\n",
    "        write.writerow([texts_filenames[i], str(texts_length[i]), count, lemmas_string])\n",
    "        \n",
    "        i += 1  # We loop through all the items in the texts_filenames: we use i to access the right lists. i increases with every iteration\n",
    "    \n",
    "\n",
    "# Now, having several lists is one way to store metadata for a list. We can also use pandas dataframes for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e6627-c95a-40db-80af-fb933eb55e44",
   "metadata": {},
   "source": [
    "### 3. Using pandas dataframe for creating a csv (optional)\n",
    "\n",
    "We can also read our lists into a pandas dataframe, where it is possible to perform further calculations or visualize the data.\n",
    "\n",
    "We have more columns now (cf. Series in the previous exercise, with only one list), so we create a Pandas DataFrame (i.e. several Series). Pandas is also used for writing our csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85de4606-1ffb-4e6c-adb3-6f462b05d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Filename\n",
      "0     001-Drucker-Philosophy.txt\n",
      "1  002-Howard-Interpretative.txt\n",
      "2        003-VandeCreek-Webs.txt\n",
      "3        004-Patrik-Encoding.txt\n",
      "4          005-Wolff-Reading.txt\n",
      "5           006-Raben-Tenure.txt\n",
      "6       007-Flanders-Welcome.txt\n",
      "7      008-Raben-Introducing.txt\n",
      "8         009-Jerz-Somewhere.txt\n",
      "9                010-Eve-All.txt\n"
     ]
    }
   ],
   "source": [
    "# Let's first import pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# There are several ways to read lists into a pandas dataframe, e.g. using dictionaries. We do the reading by simply adding new columns to our dataframe.\n",
    "\n",
    "# We create a pandas dataframe, add our list texts_filenames as the first column and name it \"Filename\"\n",
    "\n",
    "lemmas_df = pd.DataFrame(texts_filenames, columns=[\"Filename\"])\n",
    "\n",
    "print(lemmas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cd17f0c-e9cb-4e4e-9615-ba13c6f9afdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Filename  Text length\n",
      "0     001-Drucker-Philosophy.txt        15012\n",
      "1  002-Howard-Interpretative.txt        67395\n",
      "2        003-VandeCreek-Webs.txt        59335\n",
      "3        004-Patrik-Encoding.txt        33031\n",
      "4          005-Wolff-Reading.txt        40675\n",
      "5           006-Raben-Tenure.txt         4705\n",
      "6       007-Flanders-Welcome.txt         6356\n",
      "7      008-Raben-Introducing.txt         7594\n",
      "8         009-Jerz-Somewhere.txt       128041\n",
      "9                010-Eve-All.txt        92290\n"
     ]
    }
   ],
   "source": [
    "# We can insert more columns with insert()\n",
    "\n",
    "# Here we insert the next list, \"texts_length\" to our dataframe. With loc, we define where we want our column (0 -> first column, 1 -> last in his case)\n",
    "\n",
    "lemmas_df.insert(loc=1, column=\"Text length\", value = texts_length)\n",
    "\n",
    "print(lemmas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc4742f6-8a86-4188-8d03-17f52f5819a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Filename  Text length  \\\n",
      "0     001-Drucker-Philosophy.txt        15012   \n",
      "1  002-Howard-Interpretative.txt        67395   \n",
      "2        003-VandeCreek-Webs.txt        59335   \n",
      "3        004-Patrik-Encoding.txt        33031   \n",
      "4          005-Wolff-Reading.txt        40675   \n",
      "5           006-Raben-Tenure.txt         4705   \n",
      "6       007-Flanders-Welcome.txt         6356   \n",
      "7      008-Raben-Introducing.txt         7594   \n",
      "8         009-Jerz-Somewhere.txt       128041   \n",
      "9                010-Eve-All.txt        92290   \n",
      "\n",
      "                                              Lemmas  \n",
      "0  [philosophy, review, computing, fledgling, dis...  \n",
      "1  [pedagogy, quest, genre, gaming, activity, str...  \n",
      "2  [significance, history, product, type, library...  \n",
      "3  [year, tradition, philosophy, meditation, effo...  \n",
      "4  [researcher, humanity, effort, use, computer, ...  \n",
      "5  [launching, journal, status, community, public...  \n",
      "6  [issue, access, journal, issue, time, making, ...  \n",
      "7  [issue, feature, issue, issue, end, road, year...  \n",
      "8  [adventure, introduction, computer, game, prog...  \n",
      "9  [term, fiction, meaning, context, computer, st...  \n"
     ]
    }
   ],
   "source": [
    "# Finally, we insert our lemmas to the dataframe with insert()\n",
    "\n",
    "lemmas_df.insert(loc=2, column=\"Lemmas\", value = texts_noun_lemmas)\n",
    "\n",
    "print(lemmas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb578fe8-44e8-48bb-ab88-ad9e72359d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Filename  Text length  \\\n",
      "0     001-Drucker-Philosophy.txt        15012   \n",
      "1  002-Howard-Interpretative.txt        67395   \n",
      "2        003-VandeCreek-Webs.txt        59335   \n",
      "3        004-Patrik-Encoding.txt        33031   \n",
      "4          005-Wolff-Reading.txt        40675   \n",
      "5           006-Raben-Tenure.txt         4705   \n",
      "6       007-Flanders-Welcome.txt         6356   \n",
      "7      008-Raben-Introducing.txt         7594   \n",
      "8         009-Jerz-Somewhere.txt       128041   \n",
      "9                010-Eve-All.txt        92290   \n",
      "\n",
      "                                              Lemmas  \n",
      "0  philosophy review computing fledgling discipli...  \n",
      "1  pedagogy quest genre gaming activity structure...  \n",
      "2  significance history product type library reso...  \n",
      "3  year tradition philosophy meditation effort ma...  \n",
      "4  researcher humanity effort use computer techno...  \n",
      "5  launching journal status community publication...  \n",
      "6  issue access journal issue time making effort ...  \n",
      "7  issue feature issue issue end road year end ro...  \n",
      "8  adventure introduction computer game program c...  \n",
      "9  term fiction meaning context computer storytel...  \n"
     ]
    }
   ],
   "source": [
    "# The lemmas are in a list format. If we want a simpler list of strings, let's convert the lists into a string with join() method that is available in Pandas.\n",
    "# This is similar to join() we have used for lists outside Pandas, e.g. text = \" \".join(our_list)\n",
    "\n",
    "lemmas_df[\"Lemmas\"] = lemmas_df[\"Lemmas\"].str.join(\" \") # \"Lemmas\" is the name of the column, we add a \" \" between the list elements that we join together.\n",
    "\n",
    "print(lemmas_df)\n",
    "\n",
    "# Ok, now the lemmas are a string of lemmas separated with a \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7b1640c-35b1-4458-a006-fc9f2b4fcbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's write this to csv. We use the delimiter / separator \";\" again.\n",
    "# Also, we give the parameter index = False, because we do not want to write row labels (in this case, 0, 1, 2, ...)\n",
    "\n",
    "lemmas_df.to_csv(\"./lemmas_output_pandas.csv\", encoding=\"utf-8\", index = False, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a0b7f-0d5b-4723-81d7-3661ffd5baf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
