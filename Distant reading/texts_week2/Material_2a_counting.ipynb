{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0237f285-d12d-4962-936b-f1157dbac556",
   "metadata": {},
   "source": [
    "## Distant reading course week 2 (VT-23)\n",
    "\n",
    "### Learning material 2a: Counting frequencies and bigrams (and visualising the results)\n",
    "\n",
    "Matti La Mela\n",
    "\n",
    "In this learning material, we will use pandas library to count and save a csv for visualization. We can visualise the csv files either with other software (excel, RAWtools, etc.) or with python. There is optional material about basics of visualization in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fbdca4-2569-4ef7-b2a2-ed0dd0ddc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take Pride and Prejudice from Project Gutenberg with a simple http request:\n",
    "\n",
    "# NB, you can also open the URL in your browser to see that this is the right text. Remember utf-8!\n",
    "\n",
    "import requests\n",
    "\n",
    "request = requests.get(\"https://www.gutenberg.org/cache/epub/42671/pg42671.txt\")\n",
    "\n",
    "request.encoding = \"utf-8\"\n",
    "book = request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8855bd9-cfc6-4694-8e75-8fd7d0a771b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the book looks like\n",
    "\n",
    "print (len(book))\n",
    "\n",
    "print (book[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138f5f8-e3dc-40d7-a40f-663bc8c64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is quite long, so let's take the first ten chapters of the book.\n",
    "# Let's use the first and last sentence to find the offsets for the index and then store this to a new string.\n",
    "\n",
    "start = book.find(\"It is a truth universally acknowledged\")\n",
    "end = book.find(\"leaving her room for a couple of hours that evening.\")\n",
    "\n",
    "\n",
    "# We could also use CHAPTER I etc. when using find(), but be careful if there are tables of content in your file..\n",
    "\n",
    "end += len(\"leaving her room for a couple of hours that evening.\")  # we need to add this bit make the index include it too. Find returns us the location in the string where this sentence starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba531-42b9-4569-8cf5-030bf9f5ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign this slice of ten first chapters to the variable chapters\n",
    "\n",
    "chapters = book[start:end]\n",
    "\n",
    "# Let's print that it looks ok:\n",
    "\n",
    "print(chapters[0:100])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(chapters[-100:]) # -100 starts from the 100 chars before the end of the string\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c99697-a765-4820-9834-87de91b0ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# We do a bit of cleaning, if we would continue our analysis in Python only .. as we know, Spacy is very helpful with cleaning. But:\n",
    "\n",
    "chapters_clean = chapters.lower() # lowercase\n",
    "chapters_clean = chapters_clean.replace(\"\\n\", \" \") # replace endlines with \" \", if there is an extra space in some txt-type, then we could remove the endlines\n",
    "chapters_clean = re.sub(r\"[^a-z0-9\\s]\", \"\", chapters_clean) # we replace everything else than a-z 0-9 and whitespace \\s (regex character for whitespace) with \"\".\n",
    "tokens = chapters_clean.split() # we split this into a list\n",
    "\n",
    "\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0a233-f000-4966-adc7-2fce5ac21d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can count elements on the list with the count() method\n",
    "\n",
    "print(tokens.count(\"she\"))\n",
    "print(tokens.count(\"he\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c668be8-d32c-4ac2-b55b-5a6fcf60aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort the list, and have a look how it looks like.\n",
    "\n",
    "tokens_sorted = tokens\n",
    "tokens_sorted.sort()\n",
    "\n",
    "print(tokens_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e2f42-9ca9-4912-a189-5f1ff6b76bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more counting, it is possible to use Counters, which is a collection or a container for counting elements part of our variables.\n",
    "\n",
    "# We could also take this work list to excel for instance, and continue counting and visualization there\n",
    "\n",
    "# However, we will do more counting with Pandas dataframes in the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d80c9-2855-45a6-bf9b-8a7599010118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's continue with spacy for some operations: we import spacy, load the language model, and process our text into a Spacy Doc object called here part1_doc\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "chapters_doc = nlp(chapters, disable=[\"parser\", \"ner\"])  # We disable the parser and ner processes with are part of the Spacy nlp pipeline to gain some speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17729e-3042-4cc2-9e83-3b785b5e6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the lemmas and non-stop words only; and have a look how the \"trash\" looks like\n",
    "\n",
    "# we use \"and\" in the if statement, so when token.is_alpha and token.is_stop == False (thus is not a stopword), we same the lemma on our list.\n",
    "# Otherwise (else) this is store to cleaned_tokens list\n",
    "\n",
    "lemmas = []\n",
    "cleaned_tokens = []\n",
    "\n",
    "for token in chapters_doc:\n",
    "    if token.is_alpha and (token.is_stop == False):\n",
    "        lemmas.append(token.lemma_)\n",
    "    else:\n",
    "        cleaned_tokens.append(token)\n",
    "                             \n",
    "print(lemmas[0:200])\n",
    "\n",
    "# you can see how the non-alphanumerical & stopwords look like:\n",
    "\n",
    "# print(cleaned_tokens[0:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c021950-270f-4162-b48b-96281696e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this to a file, so we can open it with Excel, Antconc Voyant tools, or similar for more analysis and visualization\n",
    "\n",
    "with open(\"./texts_week2/output_lemmas.txt\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    for lemma in lemmas:\n",
    "        file.write(lemma)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a67121-cc9b-400e-8d96-d6ae5af67ff5",
   "metadata": {},
   "source": [
    "### 2. Data in tabular format (pandas)\n",
    "\n",
    "In this exercise with use only Pandas Series which contain only one list, whereas pandas dataframes can contain several lists. You can compare this to excel sheet, where you have only one column (pandas Series) or several columns (pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724ee4f-a28d-4d2e-a37b-6e694c2d4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do some basic calculations in Python about word frequencies. For this we use pandas where we can handle numbers in tables.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee610a96-009b-4a34-b063-e3246def9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We organise our list of lemmas (that we created in the previous section) into an array with Pandas\n",
    "\n",
    "lemmas_series = pd.Series(lemmas, name=\"chapters_lemmas\")\n",
    "\n",
    "# If you want, you can check the type of our object. We see that we have a Pandas series here.\n",
    "\n",
    "# print(type(lemmas_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94ef1e-ef26-4eaa-b5a8-87f00273cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we print the Series we get only the first and the last entries, which makes it easy to study\n",
    "\n",
    "print(lemmas_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8f00c-962e-4c8c-9f23-d7fc015dc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method value_counts() of the Series will return another Series where all same values have been summed up: we will get frequencies\n",
    "\n",
    "lemmas_count = lemmas_series.value_counts()\n",
    "\n",
    "print(lemmas_count)\n",
    "\n",
    "# Again, we have only one list so this is a Series. The index\n",
    "\n",
    "# print(type(lemmas_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779798e-cd09-4c09-a0cf-81dc733d583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the term index refers to the names of the terms which we had on the original list:\n",
    "\n",
    "print(lemmas_count.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaa7fb-64da-4d62-968f-35df2f56f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They correspond to list of values, which can be printed when they are converted to a list. Here we print the ten first values, thus which are the frequencies of\n",
    "# \"Bingley\", \"say\", \"Miss\", \"Elizabeth\" ..\n",
    "\n",
    "print(list(lemmas_count)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f78ae-9f7f-4b17-af0b-098e5d325151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the frequencies of our lemmas: we can open csv in excel for instance for further operations!\n",
    "\n",
    "lemmas_count.to_csv(\"texts_week2\\lemmas.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fabc48-96fb-4970-88a2-6a7c0bb81097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time, we can have a look at the 20 most common terms:\n",
    "\n",
    "print(lemmas_count[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7076c-5155-4c3a-98d2-b3154ac4472e",
   "metadata": {},
   "source": [
    "### 3. Counting bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5ceb1-4ff9-4051-8262-89f3e99177b8",
   "metadata": {},
   "source": [
    "Bigrams (and ngrams) are sequences of two words (bi) or n-words (ngram). They are useful for studying how the words occur together. There are many applications for ngrams, eg. for predicting word occurrence or building single entities when two words should occur together (eg. better to have New York for analysis (New_York) than New and York separately.\n",
    "\n",
    "The bigrams (2-gram) of the sentence \"The weather is very good\" are:\n",
    "\n",
    "- The weather\n",
    "- weather is\n",
    "- is very\n",
    "- very good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b410-3179-4220-a85a-4e619727c180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We cannot continue with our lemmas list, while we need stopwords for building the bigrams\n",
    "\n",
    "# Let's process our spacy_doc again! This time we won't remove stopwords.\n",
    "\n",
    "# In this example we do not store the non-alphanumerical in the \"else\" part. We have saved them previously for seeing what is removed,\n",
    "# if there are errors etc.\n",
    "\n",
    "tokens_cleaned = []\n",
    "tokens_lemma = []\n",
    "\n",
    "for token in chapters_doc:\n",
    "    if token.is_alpha:\n",
    "        tokens_lemma.append(token.lemma_)\n",
    "        \n",
    "print(tokens_lemma[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2f3b9-b5d6-4d4e-a093-091e2de8f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use nltk library, which has powerful tools for basic NLP operations. nltk.bigrams() turns a string into bigrams, which we save as a list to variable\n",
    "# token_bigrams\n",
    "\n",
    "import nltk\n",
    "\n",
    "token_bigrams = list(nltk.bigrams(tokens_lemma))\n",
    "\n",
    "# This is a list containing lists. We can access the elements of the list in the list by two brackets:\n",
    "\n",
    "print(token_bigrams[0:50])  # prints two first entries in our list token_bigrams\n",
    "\n",
    "# print(token_bigrams[0][0]) # prints the first element in of the first element in our list token_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0c2f2-f90f-4b1c-b4e0-6ce55a3a344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We process our list of lists a bit, and make it into a list of strings, where the bigram elements are combined. This is easier to operate.\n",
    "# eg. [('it', 'be'), ('be', 'a')] into a list of strings\n",
    "# -> [\"it be\", \"be a\"\n",
    "\n",
    "bigrams = []\n",
    "\n",
    "for bigram in token_bigrams:\n",
    "    bigrams.append(bigram[0] + ' ' + bigram[1])\n",
    "\n",
    "print(bigrams[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2b601-795c-41ad-9176-db65a8b30724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use our bigrams for simple analysis. Can we find any differences between uses of \"he\" and \"she\"\n",
    "# in the text when look at the bigrams?\n",
    "\n",
    "# We use the regex \\bhe\\b for capturing \"he\" and \\bshe\\b for capturing \"she\". \\b marks word boundary. NB we need two \\\\ while the python syntax removes the \\ in this\n",
    "# string operation.\n",
    "#\n",
    "# We want to capture also possession bigrams, eg. his and her. For his and her, we take only bigrams where the \"his\" is the first word of the bigram.\n",
    "\n",
    "\n",
    "he_bigrams = []\n",
    "she_bigrams = []\n",
    "\n",
    "his_bigrams = []\n",
    "her_bigrams = []\n",
    "\n",
    "for bigram in bigrams:\n",
    "    if re.search(\"\\\\bhe\\\\b\", bigram):     # \\\\b -> \\b\n",
    "        he_bigrams.append(bigram)\n",
    "    if re.search(\"\\\\bshe\\\\b\", bigram):\n",
    "        she_bigrams.append(bigram)\n",
    "    if re.search(\"\\\\bhis\\\\b\", bigram.split()[0]):   # we split the bigram into two, and do the searching only concerning the first bigram word\n",
    "        his_bigrams.append(bigram)\n",
    "    if re.search(\"\\\\bher\\\\b\", bigram.split()[0]):   # we split the bigram into two, and do the searching only concerning the first bigram word\n",
    "        her_bigrams.append(bigram)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f25519-4bae-49ae-9128-4c2d198d3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take them to pandas Series and calculate directly the frequencies with value_counts()\n",
    "\n",
    "\n",
    "he_bigrams_count = pd.Series(he_bigrams, name = \"he\").value_counts()\n",
    "she_bigrams_count = pd.Series(she_bigrams, name = \"she\").value_counts()\n",
    "his_bigrams_count = pd.Series(his_bigrams, name = \"his\").value_counts()\n",
    "her_bigrams_count = pd.Series(her_bigrams, name = \"her\").value_counts()\n",
    "\n",
    "# we can have a look at the he & his terms. \n",
    "\n",
    "print(he_bigrams_count[:40])\n",
    "print(\"******************\")\n",
    "print(she_bigrams_count[:40])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa38ff0-3db0-425b-8733-3369c83d6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about the his and her bigrams?\n",
    "\n",
    "print(his_bigrams_count[0:50])\n",
    "print(\"****************\")\n",
    "print(her_bigrams_count[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169889ed-2d3b-4803-9f88-cac21ab807a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the output\n",
    "\n",
    "his_bigrams_count.to_csv(\"texts_week2/bigrams-his.csv\", encoding=\"utf-8\")\n",
    "\n",
    "her_bigrams_count.to_csv(\"texts_week2/bigrams-her.csv\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783a15a-1157-4e90-be7c-9d4d5c07ffc1",
   "metadata": {},
   "source": [
    "Try to open the csv in Excel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736be5c1-f0c1-44ad-8f8e-8ea310c79ff1",
   "metadata": {},
   "source": [
    "### 4. Visualization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9aca9-3ddf-4353-9e95-986df56ef014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The library matplotlib enables visualisations. We can do this also directly with pandas, which uses matlotlib too.\n",
    "#\n",
    "# Here is a simple example for visualising the 50 most common bigrams\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bigrams_her = her_bigrams_count[0:50]\n",
    "\n",
    "# we plot now our bigrams on a bar chart:\n",
    "\n",
    "# we define the size of the figure\n",
    "plt.figure(figsize = (15, 5)) \n",
    "\n",
    "# we put \"index\" values thus the bigrams on x-axis, and the count values (list) to y-axis, and define our bar chart color as \"green\"\n",
    "plt.bar(bigrams_her.index, bigrams_her.tolist(), color=\"green\")\n",
    "\n",
    "# we rotate the x-axis labels by 90 so we can read them   \n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# we give a label to the x axis\n",
    "plt.xlabel(\"Bigrams\")\n",
    "\n",
    "# we give a label to the y axis\n",
    "plt.ylabel(\"Frequency (n)\")\n",
    "\n",
    "# we give a title to our figure\n",
    "plt.suptitle(\"50 most frequent lemmatized bigrams with 'her' as the first word (Pride and prejudice, ch 1-10)\")\n",
    "\n",
    "# the figure is saved to our material folder\n",
    "plt.savefig(\"./texts_week2/bigrams.png\", dpi = 200)\n",
    "\n",
    "# we display the figure in Jupiter Lab\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ca40-ab7d-4bea-a950-1bc66fb1aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualize our lemmas in a wordcloud; you need to install the wordcloud package first by using pip!\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(background_color=\"black\",width=500,height=500, max_words=50).generate_from_frequencies(lemmas_count)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./texts_week2/wordcloud.png\", dpi = 200)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57576c9f-a813-4897-a3b5-01aab8675a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
