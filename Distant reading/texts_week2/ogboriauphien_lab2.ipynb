{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T2) Find a regular expression that matches dates that are in the format “YYYY-MM-DD”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first input date string is 28-01-2023\n",
      "Matching both the date input if it's in the same format or not: None\n"
     ]
    }
   ],
   "source": [
    "#importing re functions\n",
    "import re\n",
    "#storing the value of datestring in a variable\n",
    "date = '28-01-2023'\n",
    "#use re.match() functions to match the datestring\n",
    "str =re.match('(\\d{2})[/.-](\\d{2})[/.-](\\d{4})$', date)\n",
    "#printing the str.group()\n",
    "print (\"The first input date string is\", str.group())\n",
    "#again declaring the datestring variable with different date format\n",
    "date = '2022-08-31'\n",
    "#matching the datestring with re.match() functions.\n",
    "str=re.match('(\\d{2})[/.-](\\d{2})[/.-](\\d{4})$', date)\n",
    "#printing the str\n",
    "print (\"Matching both the date input if it's in the same format or not:\", str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T3) Use re.findall() in Python for finding all references in the article ”dhq-2018-000396-Foka-Introduction.txt”. Write a code that opens the txt file in python, find the references with re.findall() and print the results list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incorporeal, unembodied, and disembodied.', 'environmental, social or ecological terms.', 'geospatial, time-interactive maps.', 'or, for example, sensory data.', 'studying, or producing digital textual objects.', 'performance, and the cultural aspects at play.', 'theory, practice and research data infrastructure.', 'with, to paraphrase Levi Strauss.', 'transduction, the conversion of energy from one form to another.', 'Foka, A., Westin, J. 2016.', '4, Taylor and Francis.', 'Terras, M. 2009.', 'Cyberinfrastructure, Digital Humanities Quarterly 3:1.', 'Woolford, K. Norman, S.', 'Humanities, Oxford University Press.', 'Worlds, New York: HN Abrams.', 'Music, Machines, and Experience.', 'Buckland, P. 2014.', 'Kult, Humbolt University.', 'together, in H.', 'Schreibman, S., Siemens, R., Unsworth, J. 2004.', 'Uncertainty, Visual Anthropology Review(pp. 139–150).', 'Routledge, 24, volume 3: 283–286.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./text/dhq-2018-000396-Foka-Introduction.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    article = file.read()\n",
    "\n",
    "references = re.findall(r'(\\w+, \\w.+\\.)', article)\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5) Compare the txt and the actual article and discuss briefly what differences you find, and how this might affect your analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I couldn't tell the differences in both files aside the structure(i could be wrong, perharps you can share more light on it)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T6) What is the word/term you selected? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T7) How many hits of the term are there in the corpus? How does the search change if you truncate it (eg. shorten it for searches with wildcards, regular expressions)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: the following were for ALL HITS\n",
    "### Answer: - There are over 1710 hits on the word \"corpus\"\n",
    "### - with the Regular expressions turned on i got 1806 hits\n",
    "### - For the wildcard i used \"corpus a*\" and i got 213 hits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T8) In how many different documents can you find the term? Do they appear at some particular timespan in the corpus? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I found the term in 196 files. i used the plot to target the hits\n",
    "### It appears that the word appeared frequently in the begining and at the ending of a document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T9) Copy-paste the first five concordance lines in this Lab assignment cell. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dhq-2016-000229-Duhaime-Textual.txt\tthe 1 Million English Google Ngrams \tcorpus and \tthe All English \n",
    "\n",
    "dhq-2020-000490-Jimenez-Badillo-Developing.txt\tthe orthographic variance across the \tcorpus and \tthe high frequency \n",
    "\n",
    "dhq-2012-000136-Gibbs-Building.txt\tMark Davies's Time Magazine \tcorpus (and \tthe interface to \n",
    "\n",
    "dhq-2019-000436-Ravenscroft-Finding.txt\tincluding the size of the \tcorpus and \tthe pragmatic goals \n",
    "\n",
    "dhq-2014-000168-Strange-Mining.txt\tthe modest size of the \tcorpus and \tthe research funding \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T10) Which are the five words that are statistically most likely to co-occur with your chosen term, when minimum collocate frequency is set to 5 and the size of the word window is plus minus 5 words (Window span: 5L to 5 R)? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### the five words that are statistically most likely to occur with the word \"corpus\" (sort by likelihood) are \n",
    "\n",
    "- sub is appearing on the left(freqL) 68 times and on the right (freqR) 6 times\n",
    "- linguistics is appearing on the left(freqL) 9 times and on the right (freqR) 65 times\n",
    "- corpus is appearing on the left(freqL) 36 times and on the right (freqR) 36 times\n",
    "- a is appearing on the left(freqL) 417 times and on the right (freqR) 182 times\n",
    "- texts is appearing on the left(freqL) 29 times and on the right (freqR) 55 times\n",
    "\n",
    "\n",
    "### However the five words that are statistically most likely to occur with the word \"corpus\" (sort by effects) are \n",
    "\n",
    "- enb is appearing on the left(freqL) 6 times and on the right (freqR) 2 times\n",
    "- kantrowitz is appearing on the left(freqL) 9 times and on the right (freqR) 0 times\n",
    "- eletrônico is appearing on the left(freqL) 0 times and on the right (freqR) 6 times\n",
    "- herbal is appearing on the left(freqL) 3 times and on the right (freqR) 2 times\n",
    "- texts is appearing on the left(freqL) 10 times and on the right (freqR) 2 times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T11) How does the output change when you raise the minimum collocate frequency to 10? Discuss briefly your results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the minimum frequency was raised to 10, sort by likelihood remained the same, i would assume that this is because no matter the frequency the likelihood span remains the same.\n",
    "\n",
    "### However the sort by effect changes when the minimum frequency were changed to 5. \n",
    "\n",
    "- Ridges became ranked 1 with over 200 freq and 10 freqL and 2 freqR and 8006 effect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocate\tRank\tFreq(Scaled)\tFreqLR\tFreqL\tFreqR\tRange\tLikelihood\tEffect\n",
    "\n",
    "ridges\t1\t200\t12\t10\t2\t3\t94.636\t7.077\n",
    "\n",
    "vaderland\t2\t360\t14\t13\t1\t1\t98.076\t6.451\n",
    "\n",
    "sub\t3\t2880\t74\t68\t6\t10\t457.186\t5.853\n",
    "\n",
    "symphony\t4\t540\t12\t10\t2\t1\t70.634\t5.644\n",
    "\n",
    "linguistics\t5\t3570\t74\t9\t65\t33\t425.637\t5.543\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T12) How does the output change when you raise the size of the word window to plus minus 8 words? Discuss briefly your results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the output changes again(min. freq 10, window span:8L to 8L ) making the word kantrowitz the most effect word in the corpus with over 6758 effect and 74,347 likelihood \n",
    "\n",
    "Collocate\tRank\tFreq(Scaled)\tFreqLR\tFreqL\tFreqR\tRange\tLikelihood\tEffect\n",
    "\n",
    "kantrowitz\t1\t208\t10\t9\t1\t1\t74.347\t6.758\n",
    "\n",
    "ridges\t2\t320\t14\t12\t2\t3\t101.410\t6.621\n",
    "\n",
    "vaderland\t3\t576\t23\t17\t6\t1\t162.363\t6.490\n",
    "\n",
    "bier\t4\t544\t10\t6\t4\t1\t55.114\t5.370\n",
    "\n",
    "sub\t5\t4608\t82\t74\t8\t11\t446.897\t5.324\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T13) Think about filtering of the corpus (eg. removing stop words) that would be relevant for improving the results or focusing your analysis. Motivate your choice of filtering. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there are several methods of filtering the corpus but i would say Remove irrelevant data using spacy tokenization, such as stop words, numbers, and punctuation, to improve the accuracy and efficiency of the analysis. \n",
    "\n",
    "- another method is we can specify relevant criteria for filtering the text, such as keywords, themes, or topics of interest, to ensure that we are focusing on the most relevant information.\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T14) Which 10 terms are most represented for the first five years of DHQ (Choose the articles published in 2007–2011 as your target corpus)? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- game\n",
    "\t\n",
    "- cave\n",
    "\t\n",
    "- crowther\n",
    "\t\n",
    "- player\n",
    "\t\n",
    "- bush\n",
    "\t\n",
    "- stein\n",
    "\t\n",
    "- if\n",
    "\t\n",
    "- avatar\n",
    "\t\n",
    "- computing\n",
    "\t\n",
    "- the\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T15) Which 10 terms are most represented for the last five years in DHQ corpus (Choose the articles published in 2016–2020 as your target corpus? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- comics\n",
    "\n",
    "- electronic\n",
    "\n",
    "- stein\n",
    "\n",
    "- kindle\n",
    "\n",
    "- amazon\n",
    "\n",
    "- coetzee\n",
    "\n",
    "- hawthorne\n",
    "\n",
    "- you\n",
    "\n",
    "- the\n",
    "\n",
    "- s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T16) Discuss briefly whether these results can tell us something about the development of the field of DH (as represented by the DHQ) from the 2000s until late 2010s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of different words reports from 2007-2010 and 2016-2020 shows a change in what digital humanities is focusing on. In the earlier years, they were more focused on digital gaming and virtual worlds. In later years, they started looking more at how digital technology affects cultural production and consumption, like comic books, electronic devices, and online retail. This change in language shows how the field has evolved and expanded."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T17) Create a corpus of your own. Remember that the files need to be in txt-format and that the documents should be somewhat homogenic (eg. similar kind and belonging together). You need to have at least 10 txt files in your corpus. Describe briefly what your corpus is, how you created it, and for what (hypothetical) research aim? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS: The corpus I created is all billboard pop top 100 hit songs lyrics from 2005 to 2015\n",
    "### AIM:  analyze the trends and themes in popular music during that time period, to gain insights into cultural and societal influences, or to study changes in language use and expression in music over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T18) Conduct analysis by using the following in Antconc: \n",
    " \n",
    "#       1) Concordances / KWIC "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I searched for the word \"love\" and got 3041 hits \n",
    "\n",
    "2010.txt\tlove yeahyour love your love yeah your love your love yeahyour \tlove \tyour love yeahyour love your loveyour love your loveyour \n",
    "\n",
    "2010.txt\tfried i just cant get you off my mind because your \tlove \tyour love your love is my drug your love \n",
    "\n",
    "2012.txt\tside as your shadow crosses mine mine mine mine we found \tlove \tin a hopeless place we found love in a \n",
    "\n",
    "2010.txt\tbaby baby yeah take it take it yeah love me \tlove \tme yea hi like the way you touch me there \n",
    "\n",
    "2013.txt\tto let you know you areyou are you are the \tlove \tof my life girl youre my reflection all i see \n",
    "\n",
    "2014.txt\tthe outcast or be the backlash of somebodys lack of \tlove \tor you can start speaking up nothings gonna hurt \n",
    "\n",
    "2013.txt\tthan empty sheets between our love our love oh our \tlove \tour love just give me a reason just a little \n",
    "\n",
    "2008.txt\twhen she go out she shut down the whole set \tlove \tto see her in heels wit the slit in \n",
    "\n",
    "2005.txt\twhen its all said and done guess im still in \tlove \twith you now the truth is it hurts but i \n",
    "\n",
    "2010.txt\tlet me love you down theres so many ways to \tlove \tya baby i can break you down theres so \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](love.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   2) Collocates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I ran the collocates of the word love with sort by: likelihood,window span plus and minus 5 and the minimum frequency set to 5. my result on the first 10 hits was this: \n",
    "\n",
    "- love \n",
    "- runaway\n",
    "- lockdown\n",
    "- yeahyour \n",
    "- your\n",
    "- you\n",
    "- me\n",
    "- coco \n",
    "- harder \n",
    "- hopeless\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](collocate.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T19) Discuss briefly your findings (show the results, e.g. screenshot, copy-paste or describe). What could be possible ways for broadening / deepening your study?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS: The data(text format) i got was not so clean but what i realize from my the results i got was that the word \"oh\" \"up\" \"yeah\" were the most frequently used words when i analyzed the text using the KEYWORD function(sort by effects).\n",
    "\n",
    "### Potential Research: \n",
    "- broadening the data to compare perhaps song lyrics from the 80s and 90s\n",
    "- cleaning the data and structuring it to categorize male and female artist and see the word frequently used between the old and modern billboard charts to see how the structure and content of lyrics have changed with time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T20) In Python (Notebook), open a novel that interests you from Project Gutenberg (www.gutenberg.org) (other than the novels used in the learning material). You can either download the txt or use request.get directly (find the URL of the txt in your browser). Read this into a string variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Fifty Famous People, by James Baldwin\n",
      "\n",
      "Copyright laws are changing all over the world. Be sure to check the\n",
      "copyright laws for your country before downloading or redistributing\n",
      "this or any other Project Gutenberg eBook.\n",
      "\n",
      "This header should be the first thing seen when viewing this Project\n",
      "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
      "header wit\n"
     ]
    }
   ],
   "source": [
    "#opening the txt file: fifty famous people\n",
    "with open(\"./Fifty famous people.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    book = file.read()\n",
    "first_part = book[0:400]\n",
    "print(first_part)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T21) Selected only some shorter passage of the book (a chapter, or a part of the book) and assign it to a string variable called “text_short”. Print 500 first characters of the variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the first chapter\n",
    "start = book.find(\"One day in spring four men were riding on horseback along a country\")\n",
    "end = book.find(\"noble conduct, filled their pockets with gold.\") + len(\"noble conduct, filled their pockets with gold.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day in spring four men were riding on horseback along a country\n",
      "road. These men were lawyers, an\n",
      "\n",
      "e two young princes to him, and as a reward for their\n",
      "noble conduct, filled their pockets with gold.\n",
      "\n",
      "81316\n"
     ]
    }
   ],
   "source": [
    "text_short = book[start:end]\n",
    "\n",
    "# printing the first 500 characters\n",
    "\n",
    "print(text_short[0:100])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(text_short[-100:]) # -100 starts from the 100 chars before the end of the string\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(len(text_short))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T22) Use spacy nlp() to process the “text_short” into a doc object, which you can name “text_doc”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we import spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "#load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we will take the text to spacy by calling the nlp() function\n",
    "text_doc = nlp(text_short)\n",
    "\n",
    "#to make sure the nlp is working\n",
    "type(text_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T23) Use for loop to go through the text_doc, and save all non-stop words into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day', 'spring', 'men', 'riding', 'horseback', 'country', '\\n', 'road', '.', 'men', 'lawyers', ',', 'going', 'town', '\\n', 'attend', 'court', '.', '\\n\\n', 'rain', ',', 'ground', 'soft', '.', 'Water', 'dripping', '\\n', 'trees', ',', 'grass']\n"
     ]
    }
   ],
   "source": [
    "nonstop = []\n",
    "stop = []\n",
    "\n",
    "# we use the for loop to loop through the text to find the stopwords and put them on the stop list and nonstop in the nonstop list using the .append()\n",
    "\n",
    "for token in text_doc:        \n",
    "    if token.is_stop:                     \n",
    "        stop.append(token.text)        \n",
    "    else:\n",
    "        nonstop.append(token.text)       \n",
    "\n",
    "# printing everything would be too long so lets print just 30 words for both\n",
    "print(nonstop[0:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T24) Assign this list into a Pandas pd.Series (remember to “import pandas as pd” before this). Further, calculate frequencies with value_counts() method and store the result into a list variable. Print the twenty most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           day\n",
      "1        spring\n",
      "2           men\n",
      "3        riding\n",
      "4     horseback\n",
      "5       country\n",
      "6            \\n\n",
      "7          road\n",
      "8             .\n",
      "9           men\n",
      "10      lawyers\n",
      "11            ,\n",
      "12        going\n",
      "13         town\n",
      "14           \\n\n",
      "15       attend\n",
      "16        court\n",
      "17            .\n",
      "18         \\n\\n\n",
      "19         rain\n",
      "Name: chapter_nonstop, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#convert the list to panda series\n",
    "series = pd.Series(nonstop, name=\"chapter_nonstop\")\n",
    "\n",
    "\n",
    "\n",
    "print(series[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".          1196\n",
      ",           861\n",
      "\"           816\n",
      "\\n          807\n",
      "\\n\\n        528\n",
      "           ... \n",
      "Like          1\n",
      "clear         1\n",
      "stirrup       1\n",
      "started       1\n",
      "pockets       1\n",
      "Name: chapter_nonstop, Length: 1960, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the frequency of each value using value_counts()\n",
    "frequencies = series.value_counts()\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['.', ',', '\"', '\\n', '\\n\\n', 'said', '!', '?', ';', '\\n      ',\n",
      "       ...\n",
      "       'clatter', 'beds', 'spot', 'awoke', 'loose', 'Like', 'clear', 'stirrup',\n",
      "       'started', 'pockets'],\n",
      "      dtype='object', length=1960)\n",
      "\n",
      "[1196, 861, 816, 807, 528, 164, 98, 75, 67, 58, 58, 52, 51, 48, 44, 39, 38, 37, 37, 36, 36, 34, 33, 33, 32, 32, 32, 28, 28, 27, 27, 27, 27, 26, 26, 25, 25, 25, 24, 23]\n"
     ]
    }
   ],
   "source": [
    "print(frequencies.index)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(list(frequencies)[0:40])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T25) Discuss briefly what you can say about your book based on the list. Would some other filtering have changed your results (e.g. taking alphanumerical words, or verbs, for instance). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i would suggest that the book likely focuses on the daily experiences and emotions of the main character and since the title of the book is \"Fifty Famous people\", it implies that. \"Day\" implies that the story takes place over a period of time and follows the main character's journey. \"Loved\" and \"thought\" suggest that the main character's is experiencing strong emotions and reflecting on their thoughts and feelings. \"Tired\" implies that he/she may be facing challenges or obstacles, while \"grew\" suggests that they may be undergoing personal growth or change. \"Hero\" could indicate that the protagonist is a hero in the story, or that they aspire to be one. However this is barely a deduction from examining the nonstop words and the title of the book.\n",
    "\n",
    "- I believe further word or text analysis might reveal more details about the general concept of the book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T26) Save the list you have created (words and their frequencies) into a CSV file (see Material_2B_counting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './nonstop.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Desktop\\python\\Distant reading\\texts_week2\\ogboriauphien_lab2.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/python/Distant%20reading/texts_week2/ogboriauphien_lab2.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Let's save the frequencies of our nonstop words: we can open csv in excel for instance for further operations!\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/python/Distant%20reading/texts_week2/ogboriauphien_lab2.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m frequencies\u001b[39m.\u001b[39;49mto_csv(\u001b[39m\"\u001b[39;49m\u001b[39m./nonstop.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3552\u001b[0m     path_or_buf,\n\u001b[0;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './nonstop.csv'"
     ]
    }
   ],
   "source": [
    "# Let's save the frequencies of our nonstop words: we can open csv in excel for instance for further operations!\n",
    "\n",
    "frequencies.to_csv(\"./nonstop.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T27) Open csv in Excel or other spreadsheet program and present the data as a bar chart (with 20 most common words). Include it as a image to Jupyter Notebook (eg. just copy-paste it to your Markdown cell). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For some reasons my nonstop words are all printing as \"1\" i dont know what the issue is\n",
    "\n",
    "### NOTE: I did try using the alphanumeric example you did in class and it came out great with different figures, perharps because it was nonstop words meaning they only appear once(hence 1 index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional T28) Visualize the word frequencies in Jupyter Notebook either as a chart or a word cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(range(len(frequencies)), frequencies)\n",
    "\n",
    "plt.xlabel(\"X Axis Label\")\n",
    "plt.ylabel(\"Y Axis Label\")\n",
    "\n",
    "plt.title(\"Chart Title\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
